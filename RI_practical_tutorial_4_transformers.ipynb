{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWVTcYmBQmMBucXBlNXpGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-deti-information-retrieval/Neural-IR-hands-on/blob/main/RI_practical_tutorial_4_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UzIMVvBcoWw",
        "outputId": "54919a99-17df-491c-de07-616fb5f43914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huggingface (the hub)\n",
        "\n",
        "ðŸ¤— HuggingFace ðŸ¤— is a company that has gained prominence for its significant contributions to the field of natural language processing (NLP). Founded in 2016, the company focuses on developing and providing open-source tools and technologies centered around machine learning, particularly for NLP applications.\n",
        "\n",
        "The most notable contribution of Hugging Face is its development of the \"Transformers\" library. This library is a comprehensive collection of pre-trained models and tools designed for NLP tasks. It is built on deep learning frameworks like PyTorch and TensorFlow and is well-known for its user-friendly interface and flexibility.\n",
        "\n",
        "The Transformers library includes a range of state-of-the-art models like BERT, GPT, T5, and others, which have been pre-trained on vast amounts of text data. These models can be easily adapted or fine-tuned for various NLP tasks such as text classification, question-answering, summarization, and language translation.\n",
        "\n",
        "Corrently the huggingface ecosystem is vast, the following list shows the most popular services/libraries:\n",
        "\n",
        "1. **Transformers Library**: A comprehensive library of pre-trained models for NLP tasks. [Link](https://huggingface.co/transformers/)\n",
        "\n",
        "2. **Accelerate**: Simplifies running machine learning models on any hardware configuration. [Link](https://huggingface.co/docs/accelerate/)\n",
        "\n",
        "3. **Optimum**: Optimization tools for improving performance and efficiency of machine learning models. [Link](https://huggingface.co/docs/optimum/index)\n",
        "\n",
        "4. **PEFT**: Performance Estimation Framework for Transformers, aiding in the performance analysis of Transformer models. [Link](https://huggingface.co/docs/peft/index)\n",
        "\n",
        "5. **Tokenizers**: Efficient and versatile tokenization for NLP preprocessing. [Link](https://huggingface.co/docs/tokenizers/index)\n",
        "\n",
        "6. **Datasets**: A library for easy access and sharing of NLP datasets. [Link](https://huggingface.co/docs/datasets/index)\n",
        "\n",
        "7. **Hugging Face Hub**: An online platform for sharing and collaborating on models and datasets. [Link](https://huggingface.co/docs/hub/index)\n",
        "\n",
        "8. **Inference API**: API for running machine learning models hosted on the Hugging Face Hub. [Link](https://huggingface.co/docs/api-inference/index)\n",
        "\n",
        "9. **AutoTrain**: Service for automating the training, evaluation, and deployment of NLP models. [Link](https://huggingface.co/docs/autotrain/index)\n",
        "\n",
        "10. **Spaces**: Allows users to create and share machine learning applications. [Link](https://huggingface.co/spaces)\n"
      ],
      "metadata": {
        "id": "LTVeUfTUdgen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "## Studying contextual word embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "AUwXtENVpFIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "# Auto classes are \"smart classes\" used to load already trained models, the Auto\n",
        "# class will automaticly figure out what is the correct class that should be\n",
        "# used."
      ],
      "metadata": {
        "id": "CHslSDdEdguW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
        "bert_model = AutoModel.from_pretrained(bert_checkpoint).to(\"cuda\")"
      ],
      "metadata": {
        "id": "j34Y5hYPn8AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3If8fkcIoPAp",
        "outputId": "66f08f37-a992-4bf9-98a1-3c89c660eabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUc86LMboXYQ",
        "outputId": "c52d1123-7df6-436b-b204-cc8e3186cab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_data = tokenizer([\"I am opening a bank account\",\n",
        "                       \"I am sitting on the river bank\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "tokens_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcOZEUnUongb",
        "outputId": "34dc28f0-203b-4564-f191-e1f2370296d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2572, 3098, 1037, 2924, 4070,  102,    0],\n",
              "        [ 101, 1045, 2572, 3564, 2006, 1996, 2314, 2924,  102]],\n",
              "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = \"\\n\".join([\" | \".join([f\"{tokenizer.decode(token_id)} ({i})\" for i,token_id in enumerate(_ids) ])\n",
        "                  for _ids in tokens_data.input_ids])\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQci6e3qrdYH",
        "outputId": "258182e0-bb50-48f4-bc53-ca208df77e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] (0) | i (1) | am (2) | opening (3) | a (4) | bank (5) | account (6) | [SEP] (7) | [PAD] (8)\n",
            "[CLS] (0) | i (1) | am (2) | sitting (3) | on (4) | the (5) | river (6) | bank (7) | [SEP] (8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out  = bert_model(**tokens_data)\n",
        "print(out.last_hidden_state.shape)\n",
        "print(out.pooler_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHZdgdmDpD1S",
        "outputId": "7dbfdbf7-962c-4a2d-c007-30fc30a1bfdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 9, 768])\n",
            "torch.Size([2, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bank_in_first_sentence = out.last_hidden_state[0,5]\n",
        "bank_in_second_sentence = out.last_hidden_state[1,7]"
      ],
      "metadata": {
        "id": "dg8mbXJ-piOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.cosine_similarity(bank_in_first_sentence, bank_in_second_sentence, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKuLBIMFp97r",
        "outputId": "6f672116-62aa-4a44-93d4-93ea0fee6399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3630, device='cuda:0', grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_data = tokenizer([\"I am sitting on the river side\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "side_embedding = bert_model(**tokens_data).last_hidden_state[0,7]"
      ],
      "metadata": {
        "id": "Dp3gLJN8p_Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# given the context here the work \"side\" is much closer to the word \"bank\"\n",
        "\n",
        "torch.nn.functional.cosine_similarity(side_embedding, bank_in_second_sentence, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i09r57h0qE_9",
        "outputId": "77a9cf4d-fb43-475e-8365-a5cc0bb37b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8841, device='cuda:0', grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a interaction-based transformer model"
      ],
      "metadata": {
        "id": "g-KgjyRFv_ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Easy-way (using transformer API)\n",
        "\n",
        "Reduced freedom"
      ],
      "metadata": {
        "id": "CkVvd0aQwv6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "nULUQA8QuVy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_IR = AutoModelForSequenceClassification.from_pretrained(bert_checkpoint, num_labels=2)\n",
        "bert_model_IR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHrhhqWPuaAg",
        "outputId": "1c27c612-e43d-40ac-d32d-f247fb18f0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medium-way (extending the transformer API)\n",
        "\n",
        "Recommended way"
      ],
      "metadata": {
        "id": "rPvJ2BBJyrHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel, BertModel, AutoConfig\n",
        "\n",
        "# create a class that hold the model, similar to torch.nn.module\n",
        "# internally PreTrainedModel extends torch.nn.module\n",
        "\n",
        "class MyReranker(PreTrainedModel):\n",
        "\n",
        "  def __init__(self, checkpoint):\n",
        "    config = AutoConfig.from_pretrained(checkpoint)\n",
        "    super().__init__(config)\n",
        "    # note you can also extend BertModel instead of adding it here\n",
        "    self.bert = BertModel.from_pretrained(config._name_or_path, config=config)\n",
        "    self.dropout = torch.nn.Dropout(p=0.1)\n",
        "    self.linear1 = torch.nn.Linear(768, 1024)\n",
        "    self.act = torch.nn.GELU()\n",
        "    self.linear2 = torch.nn.Linear(1024, 2)\n",
        "\n",
        "  def inference(self, *args, **kwargs):\n",
        "    with torch.no_grad():\n",
        "      logits = self(*args, **kwargs)\n",
        "      return torch.argmax(logits, dim=-1)\n",
        "\n",
        "  def forward(self,\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            token_type_ids=None,\n",
        "            position_ids=None,\n",
        "            indexes=None,\n",
        "            novel=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=None,\n",
        "            labels=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "            mask=None,\n",
        "           ):\n",
        "\n",
        "    x = self.bert(input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  token_type_ids=token_type_ids,\n",
        "                  position_ids=position_ids,\n",
        "                  head_mask=head_mask,\n",
        "                  inputs_embeds=inputs_embeds,\n",
        "                  output_attentions=output_attentions,\n",
        "                  output_hidden_states=output_hidden_states,\n",
        "                  return_dict=return_dict)\n",
        "\n",
        "    x = self.dropout(x[\"pooler_output\"])\n",
        "    x = self.linear1(x)\n",
        "    x = self.act(x)\n",
        "    return self.linear2(x)"
      ],
      "metadata": {
        "id": "Oqr2I7sbyaD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_IR = MyReranker(bert_checkpoint).to(\"cuda\")\n",
        "bert_model_IR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeb0B5Ziyqe1",
        "outputId": "19f54ad2-8b19-4316-e580-e72203f4d24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyReranker(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
              "  (act): GELU(approximate='none')\n",
              "  (linear2): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to run/infer?\n",
        "\n",
        "Similar to how we get the contextual embeddings"
      ],
      "metadata": {
        "id": "nsqee-Ko3UmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_data = tokenizer([(\"Is the Earth round?\", \"Yes, the Earth is round. It's an oblate spheroid shape due to its rotation.\"),\n",
        "                         (\"Is the Earth round?\", \"No, the Earth is not perfectly round; it's slightly flattened at the poles and bulges at the equator.\"),\n",
        "                         (\"Does water boil at 100Â°C (212Â°F) at sea level?\", \"Yes, water boils at 100Â°C (212Â°F) at sea level under standard atmospheric pressure.\"),\n",
        "                         (\"Does water boil at 100Â°C (212Â°F) at sea level?\", \"No, water does not always boil at 100Â°C if the atmospheric pressure is different, like at high altitudes.\"),\n",
        "                         (\"Is the sun a star?\", \"Yes, the sun is a star. It's a massive, luminous sphere of hot plasma at the center of our solar system.\"),\n",
        "                         (\"Is the sun a star?\", \"No, the sun is not just any star; it is a G-type main-sequence star, which is relatively rare in the universe.\")\n",
        "                         ], return_tensors=\"pt\", padding=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "zqioiKI402S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_IR(**tokens_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTM49Gri2Not",
        "outputId": "4ebdd145-70e0-4c42-ad41-ebffc13f4075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1894, -0.1369],\n",
              "        [-0.0130, -0.1273],\n",
              "        [-0.1051, -0.1360],\n",
              "        [-0.1499, -0.2182],\n",
              "        [-0.1553, -0.1602],\n",
              "        [-0.1029, -0.0983]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_IR.inference(**tokens_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7U5d7ym7OeG",
        "outputId": "4d41598a-3acb-4bbb-e3d7-69423dcc9001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 1, 1, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzYhdoJK-daH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}