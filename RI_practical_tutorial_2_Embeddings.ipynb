{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFHoHG0YBTVGJOjBZDdjIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-deti-information-retrieval/Neural-IR-hands-on/blob/main/RI_practical_tutorial_2_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RI practical tutorial #2\n",
        "\n",
        "## Embeddings\n",
        "\n",
        "An important component of natural language processing (NLP) is the ability to translate words, phrases, or larger bodies of text into continuous numerical vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "WEzDVxR_GS_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "HTM0S306OL5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7KLH66SGHFk"
      },
      "outputs": [],
      "source": [
        "!pip install torch matplotlib\n",
        "!git clone https://github.com/ua-deti-information-retrieval/Neural-IR-hands-on.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "E5CGinJe1IBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap\n",
        "\n",
        "Embeddings convert words, sentences, or even entire documents into vectors of real numbers. Unlike traditional methods like one-hot encoding, which represent words as isolated and high-dimensional points."
      ],
      "metadata": {
        "id": "HnYRQPKWORgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_vocab = ['the','supreme','art','of','war','is','to','subdue','the','enemy','without','fighting']\n",
        "torch.eye(len(toy_vocab))"
      ],
      "metadata": {
        "id": "FWQGAEJAlMX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = torch.nn.Embedding(len(toy_vocab), 4)\n",
        "print(embedding_layer.weight)\n",
        "print(\"embeddings norm\", torch.linalg.norm(embedding_layer.weight, ord=2, dim=-1))"
      ],
      "metadata": {
        "id": "ApSIqDxl2BDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hands on\n",
        "\n",
        "To get started with practical exercises in embeddings, it's beneficial to use pre-trained models. This allows us to explore and understand the power of embeddings without the need for extensive computational resources and time to train our models.\n",
        "\n",
        "For our exercise, we will use the DESM (Dual Embedding Space Model) from Microsoft (the same introduced in class). DESM is a unique model that leverages two types of embeddings."
      ],
      "metadata": {
        "id": "m8iWJcDj2y6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run to download the desm embeddings\n",
        "!wget https://download.microsoft.com/download/A/7/C/A7C7F0A6-B925-4C07-A14B-04ACF8A8E030/desm.zip\n",
        "!unzip desm.zip"
      ],
      "metadata": {
        "id": "zbztb3qROK9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
      ],
      "metadata": {
        "id": "iHyUI9cx79yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a simple vocab, because load the in and out matrices will exaust the resources\n",
        "with open(\"words_alpha.txt\") as f:\n",
        "#with open(\"simple_vocab_example.txt\") as f:\n",
        "  vocab_set = {token.rstrip() for token in f}"
      ],
      "metadata": {
        "id": "rcM6NGyD-QxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_embeddings_from_txt(path, vocab):\n",
        "  emb = {}\n",
        "\n",
        "  with open(path) as f:\n",
        "    for line in tqdm(f):\n",
        "      token, *values = line.split(\"\\t\")\n",
        "      if token in vocab:\n",
        "        emb[token] = list(map(float, values))\n",
        "\n",
        "  # separating the vocab from the embeddings\n",
        "  vocab, embedding = list(zip(*emb.items()))\n",
        "  token_to_id = {token:i for i,token in enumerate(vocab)}\n",
        "  id_to_token = {v:k for k,v in token_to_id.items()}\n",
        "\n",
        "  return token_to_id, id_to_token, torch.tensor(embedding)\n",
        "\n",
        "in_token_to_id, in_id_to_token, in_embeddings = load_embeddings_from_txt(\"in.txt\", vocab_set)"
      ],
      "metadata": {
        "id": "Pj1C9OLV2fZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the loaded embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "acwoInucNeKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape\", in_embeddings.shape)\n",
        "nurse_id = in_token_to_id[\"nurse\"]\n",
        "print(\"Token: nurse | id:\", nurse_id)\n",
        "print(\"embeddings norm\", torch.linalg.norm(in_embeddings[nurse_id], ord=2))\n",
        "print(\"nurse embedding:\",in_embeddings[nurse_id])"
      ],
      "metadata": {
        "id": "o_M_2gbU-P7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to find similar tokens with embeddings?\n",
        "\n",
        "The same way you find similar vectors with tfidf, using cosine similarity!\n",
        "\n",
        "More precisely, given the two vectors:\n",
        "\n",
        "$cos(a,b) = \\frac{\\vec{a}\\cdot\\vec{b}^T}{\\|\\vec{a}\\|\\times\\|\\vec{b}\\|}$\n",
        "\n",
        "Then, we just need to compute the cosine similaraty between $\\vec{a}$ and all of the vectors in our matrix $C$ (collection).\n",
        "\n",
        "As an example, complete the following function. It should calculate the cosine similarity between a given vector and all the collection vectors and return the most similar tokens and scores."
      ],
      "metadata": {
        "id": "KR_RiWQcSenm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_topk_similar_to(token, embeddings, token_to_id, id_to_token, topk=10):\n",
        "  \"\"\"\n",
        "  Given the token return topk similar tokens according to the cos sim between the\n",
        "  token vector and all of the embeddings vectors\n",
        "  \"\"\"\n",
        "\n",
        "  token_embedding = embeddings[token_to_id[token]]\n",
        "  return find_topk_similar_to_vec(token_embedding, embeddings, token_to_id, id_to_token, topk)\n",
        "\n",
        "def find_topk_similar_to_vec(token_embedding, embeddings, token_to_id, id_to_token, topk=10):\n",
        "  \"\"\"\n",
        "  Given the token embedding return topk similar tokens according to the cos sim between the\n",
        "  token vector and all of the embeddings vectors\n",
        "\n",
        "\n",
        "  [('mercedes', 0.9999992251396179),\n",
        "  ('cabriolet', 0.6590193510055542),\n",
        "  ('sprinter', 0.6370120048522949),\n",
        "  ('volkswagen', 0.6347604393959045),\n",
        "  ('fiat', 0.6245887875556946),\n",
        "  ('jaguar', 0.6102705001831055),\n",
        "  ('toyota', 0.5901010632514954),\n",
        "  ('honda', 0.5850051641464233),\n",
        "  ('rover', 0.5818690061569214),\n",
        "  ('freightliner', 0.5783664584159851)]\n",
        "\n",
        "  \"\"\"\n",
        "  ## complete\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQVkGG3KwWvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_topk_similar_to(\"yale\", in_embeddings, in_token_to_id, in_id_to_token)\n"
      ],
      "metadata": {
        "id": "f2W8orRdyrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_topk_similar_to(\"apple\", in_embeddings, in_token_to_id, in_id_to_token)"
      ],
      "metadata": {
        "id": "RN4q_78tz9CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_topk_similar_to(\"oak\", in_embeddings, in_token_to_id, in_id_to_token)"
      ],
      "metadata": {
        "id": "5s44FRme_LQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why it works bad for covid? any guess?\n",
        "find_topk_similar_to(\"covid\", in_embeddings, in_token_to_id, in_id_to_token)"
      ],
      "metadata": {
        "id": "uz_dCH2R_B3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word analogies\n",
        "\n",
        "Another interesting property of word embeddings is their ability to capture word analogies through geometric relationships in the vector space. This phenomenon is often illustrated by the famous example: \"king\" - \"man\" + \"woman\" ≈ \"queen\". In this case, the embeddings capture the relationship between gender roles and royal titles.\n",
        "\n",
        "With the help of the previous function, create a the vector queen by using appling the relation (\"king\"-\"man\") to \"woman\".\n",
        "\n"
      ],
      "metadata": {
        "id": "WNuiDqt90MLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def word_analogy(token_a, token_b, token_c):\n",
        "  \"\"\"\n",
        "  Performs token_a - token_b + token_c\n",
        "\n",
        "  and returns a list with the closest tokens\n",
        "\n",
        "  Note: token_a, token_b and token_c should be removed of the list\n",
        "\n",
        "  Example:\n",
        "  word_analogy(\"king\", \"man\", \"woman\")\n",
        "  [('queen', 0.6244865655899048),\n",
        " ('kings', 0.4600622057914734),\n",
        " ('prince', 0.42849528789520264),\n",
        " ('princess', 0.42579346895217896),\n",
        " ('royal', 0.41185224056243896),\n",
        " ('crown', 0.4051671624183655),\n",
        " ('princes', 0.40045303106307983),\n",
        " ('lamb', 0.3960754871368408),\n",
        " ('hamilton', 0.39465370774269104)]\n",
        "  \"\"\"\n",
        "  ## Complete\n",
        "  pass\n",
        "\n",
        "word_analogy(\"king\", \"man\", \"woman\") # expected queen"
      ],
      "metadata": {
        "id": "BHbbnNM91uS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_analogy(\"paris\", \"france\", \"portugal\") # expected lisbon"
      ],
      "metadata": {
        "id": "C5J2bm1d22fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_analogy(\"france\", \"paris\", \"lisbon\") # expected portugal"
      ],
      "metadata": {
        "id": "nTHcYyJa3tQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_analogy(\"teacher\", \"school\", \"hospital\") # expected ? (maybe doctor?)"
      ],
      "metadata": {
        "id": "YonYWmzB-jAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Okey, but if I want to use sentance or documents?\n",
        "\n",
        "In such scenarios, a straightforward approach is to average the embeddings of all tokens within a sentence. This method offers a means to condense the rich information of a sentence into a single vector.\n",
        "\n",
        "By averaging the embeddings of each word in a sentence, we create a composite representation that captures the essence of the sentence as a whole. This can then be used to compare and measure the similarity between different sentences or documents. It's a practical method, especially when dealing with small texts. Let's proceed to implement this and see how well it performs in identifying sentence similarities."
      ],
      "metadata": {
        "id": "NtTcrOmL4FE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_corpus = [\n",
        "    \"A nimble red fox leaped over a sleeping canine.\",\n",
        "    \"New York is known for its bustling city life.\",\n",
        "    \"The city of Tokyo is lively and vibrant at night.\",\n",
        "    \"The development of AI has significant implications for society.\",\n",
        "    \"Fresh vegetables and fruits are essential for a healthy diet.\",\n",
        "    \"Eating a variety of greens and fruits contributes to good health.\",\n",
        "    \"The book on the shelf is old and worn.\",\n",
        "    \"An ancient, tattered tome sits in the library.\"\n",
        "]\n",
        "\n",
        "sentence_to_id = {s:i for i,s in enumerate(sentences_corpus)}\n",
        "id_to_sentence = sentences_corpus\n",
        "#id_to_sentence = {v:k for k,v in sentence_to_id.items()}"
      ],
      "metadata": {
        "id": "R3ThKFWT4EHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def text_to_vec(text, embeddings, in_token_to_id):\n",
        "  tokens = text.lower().split()\n",
        "  return [embeddings[in_token_to_id[token]] for token in tokens if token in in_token_to_id]\n",
        "\n",
        "def sentence_embedding(text, embeddings):\n",
        "  \"\"\"\n",
        "  Give a sequence of text compute the embeddings of the sentece by averaging its token embeddings\n",
        "\n",
        "  use the function text_to_vec to convert text to vectors: text_to_vec(text, embeddings, in_token_to_id)\n",
        "\n",
        "  Out: sentence embeddings\n",
        "  \"\"\"\n",
        "  ## Complete\n",
        "\n",
        "  pass\n",
        "\n",
        "sentences_corpus_embeddings = torch.stack([sentence_embedding(sent, in_embeddings) for sent in sentences_corpus])\n"
      ],
      "metadata": {
        "id": "GUrhIkHC6SXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_embedding = sentence_embedding(\"Artificial Intelligence will shape the future of humanity.\", in_embeddings)\n",
        "find_topk_similar_to_vec(sent_embedding, sentences_corpus_embeddings, sentence_to_id, id_to_sentence, topk=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "KOz00Zzv61Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_embedding = sentence_embedding(\"The quick brown fox jumps over the lazy dog.\", in_embeddings)\n",
        "find_topk_similar_to_vec(sent_embedding, sentences_corpus_embeddings, sentence_to_id, id_to_sentence, topk=5)"
      ],
      "metadata": {
        "id": "ZQyoefdtByET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Well if it works for sentence similarity, maybe it works for retrieval?\n",
        "\n",
        "Let's apply the same example to this toy collection of documents"
      ],
      "metadata": {
        "id": "d3Iu4FOHD59C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Apples are rich in antioxidants, which help in fighting free radicals.\",\n",
        "    \"The water cycle consists of evaporation, condensation, and precipitation.\",\n",
        "    \"Recent trends in AI include advancements in deep learning and neural networks.\",\n",
        "    \"Good mental health can be maintained by regular exercise and proper sleep.\",\n",
        "    \"The Olympic Games originated in ancient Greece and have evolved over centuries.\",\n",
        "    \"Eating fruits and vegetables is essential for physical well-being.\",\n",
        "    \"Cloud formation is a key aspect of the earth's hydrological process.\",\n",
        "    \"Machine learning and AI are becoming integral in various industries.\",\n",
        "    \"Mindfulness and meditation are effective for stress management.\",\n",
        "    \"The modern Olympics include a variety of sports from track to swimming.\"\n",
        "]\n",
        "\n",
        "doc_to_id = {s:i for i,s in enumerate(documents)}\n",
        "id_to_doc = documents\n",
        "\n",
        "doc_embeddings = torch.stack([sentence_embedding(sent, in_embeddings) for sent in documents])\n"
      ],
      "metadata": {
        "id": "vLRFmGr3EHLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_embedding = sentence_embedding(\"How does the water cycle work?\", in_embeddings)\n",
        "find_topk_similar_to_vec(sent_embedding, doc_embeddings, doc_to_id, id_to_doc, topk=3)"
      ],
      "metadata": {
        "id": "lpG3dbVGEKgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_embedding = sentence_embedding(\"What is the history of the Olympic Games?\", in_embeddings)\n",
        "find_topk_similar_to_vec(sent_embedding, doc_embeddings, doc_to_id, id_to_doc, topk=3)"
      ],
      "metadata": {
        "id": "etuSTaFMEzLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DESM model\n",
        "\n",
        "Up to this point, we have primarily utilized the 'IN' embeddings of the DESM (Dual Embedding Space Model) model. Let's delve deeper into understanding and exploring this model:\n",
        "\n",
        "The DESM model is unique in its dual-embedding approach. It leverages both 'IN' and 'OUT' embeddings to enhance the representation of words and phrases.\n",
        "\n",
        "First lets load the OUT embeddings"
      ],
      "metadata": {
        "id": "L7lSSpKqRiwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that out_token_to_id and out_id_to_token should be exactly the same as in_token_id and in_id_to_token\n",
        "out_token_to_id, out_id_to_token, out_embeddings = load_embeddings_from_txt(\"out.txt\", vocab_set)\n"
      ],
      "metadata": {
        "id": "SEbammvH-qIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In continuation of what we've learned in class, we'll now calculate similarities using different combinations of embeddings from the DESM model. Namely, IN-IN, IN-OUT and OUT-OUT."
      ],
      "metadata": {
        "id": "WLjYNG7tGf5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def in_out_comparison_for_token(token, topk=10):\n",
        "\n",
        "  in_in_results = find_topk_similar_to(token, in_embeddings, in_token_to_id, in_id_to_token, topk=topk)\n",
        "  out_out_results = find_topk_similar_to(token, out_embeddings, out_token_to_id, out_id_to_token, topk=topk)\n",
        "  in_out_results = find_topk_similar_to_vec(in_embeddings[in_token_to_id[token]], out_embeddings, in_token_to_id, in_id_to_token, topk=topk)\n",
        "  print(f'|{\"IN-IN\":^25}|{\"OUT-OUT\":^25}|{\"IN-OUT\":^25}|')\n",
        "  for i in range(topk):\n",
        "    in_in_str = f'{in_in_results[i][0]} ({in_in_results[i][1]:.3f})'\n",
        "    out_out_str = f\"{out_out_results[i][0]} ({out_out_results[i][1]:.3f})\"\n",
        "    in_out_str = f\"{in_out_results[i][0]} ({in_out_results[i][1]:.3f})\"\n",
        "    print(f'|{in_in_str:^25}|{out_out_str:^25}|{in_out_str:^25}|')\n",
        "\n"
      ],
      "metadata": {
        "id": "uF4yZ-_IY75g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_out_comparison_for_token(\"yale\")\n"
      ],
      "metadata": {
        "id": "p3VWtv2pPHjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_out_comparison_for_token(\"apple\")"
      ],
      "metadata": {
        "id": "rfro6VLBPHlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DESM Retrieval\n",
        "\n",
        "Following the slides lets implement the DESM retrieval model\n",
        "\n",
        "$DESM(Q, D) = \\frac{1}{|Q|}\\sum_{q_i \\in Q}cos(q_i,D)$"
      ],
      "metadata": {
        "id": "_GK1dA25Lasl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Apples are rich in antioxidants, which help in fighting free radicals.\",\n",
        "    \"The water cycle consists of evaporation, condensation, and precipitation.\",\n",
        "    \"Recent trends in AI include advancements in deep learning and neural networks.\",\n",
        "    \"Good mental health can be maintained by regular exercise and proper sleep.\",\n",
        "    \"The Olympic Games originated in ancient Greece and have evolved over centuries.\",\n",
        "    \"Eating fruits and vegetables is essential for physical well-being.\",\n",
        "    \"Cloud formation is a key aspect of the earth's hydrological process.\",\n",
        "    \"Machine learning and AI are becoming integral in various industries.\",\n",
        "    \"Mindfulness and meditation are effective for stress management.\",\n",
        "    \"The modern Olympics include a variety of sports from track to swimming.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "2RTb5oraPHn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def desm(query, documents, topk=3):\n",
        "  \"\"\"\n",
        "  Implement the desm algorithm\n",
        "  query: text of a question\n",
        "  documents: list of documents text that make the collection\n",
        "  topk: maximum number of documents that we want to return\n",
        "\n",
        "  desm(\"How does the water cycle work?\", documents)\n",
        "  [('The water cycle consists of evaporation, condensation, and precipitation.',\n",
        "  -0.0023205685429275036),\n",
        " (\"Cloud formation is a key aspect of the earth's hydrological process.\",\n",
        "  -0.028624113649129868),\n",
        " ('Good mental health can be maintained by regular exercise and proper sleep.',\n",
        "  -0.031198585405945778)]\n",
        "  \"\"\"\n",
        "  ## COMPLETE\n",
        "\n",
        "\n",
        "  # average embeddings for the doc\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "O3IB_hjpMMb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desm(\"How does the water cycle work?\", documents) # it help?\n"
      ],
      "metadata": {
        "id": "caldWOHaPHqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desm(\"What is the history of the Olympic Games?\", documents)"
      ],
      "metadata": {
        "id": "J2ChqVTCPHsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzpHPcwWPHuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTWondNZPHxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsSXU6TyPHz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iwqx3xwiNVo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlxMGWQWly2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJO99y0HNV_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbiTPZT-NsbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NQ-ypyCNvdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}